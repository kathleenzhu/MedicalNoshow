
using DataFrames
using PyPlot

#proximal gradient method
include("proxgrad.jl")


#read data
data = readtable("MedicalNo-Show.csv");

#select no-show and features columns: no show, wait time, age, day of the week, month, scholarship, hypertension, smoke
noshowdata = data[:,[:NoShow,:WaitTime,:Age,:DayOfTheWeek,:ApptMonth, :Scholarship, :Hypertension, :Smokes]]

nowshowdata[:Sunday]
nowshowdata[:Monday]
nowshowdata[:Tuesday]
nowshowdata[:Wednesday]
nowshowdata[:Thursday]
nowshowdata[:Friday]
nowshowdata[:Saturday]

for i in 1: size(noshowdata[1])
    if nowshowdata[i,:WaitTime] == "Sunday"
        noshowdata[i,:Sunday] = 1
    if nowshowdata[i,:WaitTime] == "Monday"
        noshowdata[i,:Monday] = 1        
    if nowshowdata[i,:WaitTime] == "Tuesday"
        noshowdata[i,:Tuesday] = 1
    if nowshowdata[i,:WaitTime] == "Wednesday"
        noshowdata[i,:Wednesday] = 1
    if nowshowdata[i,:WaitTime] == "Thursday"
        noshowdata[i,:Thursday] = 1
    if nowshowdata[i,:WaitTime] == "Friday"
        noshowdata[i,:Friday] = 1    
    if nowshowdata[i,:WaitTime] == "Saturday"
        noshowdata[i,:Saturday] = 1

for row in 1:size(noshowdata[1])
      for col in 1:size(noshowdata[2])
        if  ismissing(noshowdata[row,col])
            noshowdata[row,col] = -1
        end
      end
end                                
                                
delete!(noshowdata, :DayOfTheWeek)                
                
                
#split 
function split(arr::Array)
    X = convert(Array{Float64}, arr[:,2:end])
    y = arr[:,1]
    
    #change encoding of binaries to 1,-1
    X[:,[:ApptMonth,:Scholarship,:Hypertension, :Smokes]] = 2*y[:] - 1
    y[:] = 2*y[:] - 1
    
    
    n = size(arr)[1]
    is = sortperm(rand(n))
    
    Xtrain = X[is[1:Int(4*round(n/5))],:] ## X training set
    Xtest = X[is[Int(4*round(n/5)+1):end],:] ## X test set
    ytrain = convert(Array{Int}, y[is[1:Int(4*round(n/5))]]) ## y training set
    ytest = convert(Array{Int}, y[is[Int(4*round(n/5)+1):end]]) ## y test set
    return X, y, Xtrain, Xtest, ytrain, ytest
end

#split data into train and test 
modeldata = convert(Array{Float64}, modeldata);
X, y, X_train, X_test, y_train, y_test = split(modeldata);

#Ridge regression 

w_ridge = proxgrad(QuadLoss(), QuadReg(), X_train, y_train; kwargs...)

#Hinge loss with l1 regularizer 

n = length(y_train)
Xoffset_train = [X_train ones(n)]
loss_hinge = 1/n*HingeLoss()
w_hinge_l1 = proxgrad(loss_hinge, OneReg(), Xoffset_train, y_train, stepsize=1, maxiters=50000)

#SVM

w_hinge_l2 = proxgrad(loss_hinge, QuadReg(), Xoffset_train, y_train, stepsize=1, maxiters=50000)

#Logistic loss with l1 regularizer

loss_logistic = 1/n*LogisticLoss()
w_logistic_l1 = proxgrad(loss_logistic, OneReg(), Xoffset_train, y_train, stepsize=1, maxiters=1000)

#Logistic loss with l2 regularizer

w_logistic_l1 = proxgrad(loss_logistic, OneReg(), Xoffset_train, y_train, stepsize=1, maxiters=1000)

#misclassification 
function misclassification(X, y, w)
    n = size(X,1)
    misclassified = 0
    for i in 1:n
        if(sign((w'*X[i,:])[1]) != y[i])
            misclassified += 1/n
        end
    end
    return round(misclassified, 4)
end

#misclassification for ridge regression 
mis_ridge = misclassification(X_test, y_test, w_ridge)

#misclassification for Hinge loss with l1 regularizer 
mis_hinge_l1 = misclassification(X_test, y_test, w_hinge_l1)

#misclassification for SVM
mis_hinge_l2 = misclassification(X_test, y_test, w_hinge_l2)

#misclassification for Logistic loss with l1 regularizer
mis_logistic_l1 = misclassification(X_test, y_test, w_logistic_l1)

#misclassification for Logistic loss with l2 regularizer
mis_logistic_l2 = misclassification(X_test, y_test, w_logistic_l2)
